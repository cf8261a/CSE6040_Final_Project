{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "earlier-poetry",
   "metadata": {},
   "source": [
    "## Are the most common words also the most important words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-chick",
   "metadata": {},
   "source": [
    "This may seem like a trivial question but in the world of text analytics, it might be harder than it seems to discern due in large part to the challenging nature of working with text data. The data is unstructured and therefore tedious to parse. \n",
    "\n",
    "Given the question posed above, imagine you are working with customer survey data (aka a corpus of reviews). It could be the case\n",
    "that a word appeared many times in one particular customer's survey. (i.e. the word *quality*). In the rest of the survey data however the word 'quality' may not appear again, but if that one particular review repeated 'quality' many , many times, it could erroneously seem like 'quality' is a major theme across all of the surveys.  \n",
    "\n",
    "In the following notebook, I'll be looking at **importance of words** via the number of times they appear across sentences in a corpus rather than via their raw count (how many times they occur altogether through an entire corpus). \n",
    "\n",
    "**Disclaimer**\n",
    "(My inspiration comes from CSE6040's Problem 20 in this semester's set of final practice problems (Fall 2021)). The general workflow will be similar to that of the Professor Vuduc's albeit with my own implementation of the data wrangling as it pertains to employee reviews for PNC Bank. The data comes from an HTML file that I downloaded and parsed and it is wrangled via my implementation of the word and sentence tokenizer and bag of words function that you will see shortly. \n",
    "\n",
    "At some point, I reuse the professor's generation of word id function and the code to reverse th mapping since the profesor's implementation was both trivial and elegantly written via python dictionaries. I also show how to manually get the corresponding left singular vector from the SVD that the professor gets via scipy's svds() function using numpy's linalg.svd function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "capital-lecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-upgrade",
   "metadata": {},
   "source": [
    "We'll first use beautiful soup to open and parse through an HTML file that I pulled directly from the review page for PNC on glassdoor. Once we parse it, we're going to pull all of the comments that were 'pros' or positive and store the result in a list called **outtext**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "interracial-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with HTML file\n",
    "with open(\"PNCReviews_Glassdoor.html\", \"r\") as f:\n",
    "    doc = bs4.BeautifulSoup(f, \"html.parser\")\n",
    "# Converting the doc file into a string\n",
    "as_string_doc = str(doc)\n",
    "\n",
    "# Regex for pulling out all the positive 'pro' reviews\n",
    "pattern = re.compile(r'\\\"pros\\\">([^<]*)<')\n",
    "result = pattern.findall(as_string_doc)\n",
    "\n",
    "# initializing an empty list for storing each of the reviews \n",
    "outtext = [match.lower() for match in result] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "educated-selling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 reviews in the corpus. Here are the first 3:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pension. 401k match, training and development opportunities if your time permits. actually making a difference in communities with green investing, a social bond, and other leading edge initiatives.',\n",
       " 'especially with the pandemic, they have been very flexible with remote capable employees. leadership has shown a willingness to listen to employees and change.',\n",
       " 'i was able to do some financial planning for the future such as creating a retirement plan and buying some shares at a discounted rate until i resigned then they make you pay it back by deducting it from your shares. this was a great opportunity for me to see what types of jobs i never want again my regret is that i didn’t resign sooner. i’m grateful to have worked in both the branches and corporate.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 3\n",
    "print(f'There are {len(outtext)} reviews in the corpus. Here are the first {n}:')\n",
    "outtext[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-volleyball",
   "metadata": {},
   "source": [
    "#### But this document only has 10 reviews?\n",
    "\n",
    "In the process of working with the review data, I learned quickly that, given the number of words we might expect to see in a review, the total number of words we'd be working with would grow substantially as the number of reviews increase. For demonstration purposes, I'm limiting the number of reviews to just these 10 but the idea works for larger corpuses as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-interest",
   "metadata": {},
   "source": [
    "#### Let's get the sentences out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "widespread-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractsentences(doc):\n",
    "    final = []\n",
    "    for s in doc:\n",
    "        out = re.split('\\?|\\!|\\.',s)\n",
    "        cleaned = [sent.strip(' ') for sent in out]\n",
    "        final = final + [sent for sent in cleaned if sent]\n",
    "    return final\n",
    "all_sents = extractsentences(outtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "young-stanley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pension',\n",
       " '401k match, training and development opportunities if your time permits',\n",
       " 'actually making a difference in communities with green investing, a social bond, and other leading edge initiatives',\n",
       " 'especially with the pandemic, they have been very flexible with remote capable employees',\n",
       " 'leadership has shown a willingness to listen to employees and change',\n",
       " 'i was able to do some financial planning for the future such as creating a retirement plan and buying some shares at a discounted rate until i resigned then they make you pay it back by deducting it from your shares',\n",
       " 'this was a great opportunity for me to see what types of jobs i never want again my regret is that i didn’t resign sooner',\n",
       " 'i’m grateful to have worked in both the branches and corporate',\n",
       " 'bank recognizes value of technology and ongoing training of employees',\n",
       " 'it is very focused on diversity and inclusion and offers a broad product suite to meet the needs of any business or individual',\n",
       " \"there's definitely a local community feel in this national big bank\",\n",
       " 'benefits are competitive and within the industry',\n",
       " 'pnc would certainly be a preferred employer',\n",
       " 'my best reason for working there is having the empowerment of being financial doctor for my local community and working to uncover problems and finding the right solutions to achieving financial wellness',\n",
       " 'it made me feel proud to help',\n",
       " 'they pay is decent enough i guess',\n",
       " 'decent culture',\n",
       " 'seems to care about employees',\n",
       " 'average benefits',\n",
       " 'work/life balance is respected',\n",
       " 'interacting with customers, fun co workers, good training',\n",
       " 'excellent, diverse financial institute',\n",
       " \"that listens to it's employees\",\n",
       " 'pnc is a well-established organization and pleasant to work for',\n",
       " 'they promote internal mobility with their employees, inclusive culture, and good wlb',\n",
       " 'good job security']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-currency",
   "metadata": {},
   "source": [
    "#### What are the most common words?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matched-trauma",
   "metadata": {},
   "source": [
    "The NLTK package is a great resource for working with text data. It has a lot of features such as a sentence and word tokenizer\n",
    "as well as what I am using below, a list of stopwords. I decided NOT to use the package's built in sent_tokenize() function nor its builtin word_tokenize() function mainly because I wanted to implement my own tokenization procedure. \n",
    "\n",
    "On the topic of stopwords like 'I', 'is',  and 'the', we would see these types of words appear a TON of times but do they \n",
    "really carry a lot of semantic meaning? *Probably not*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pointed-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwordlist = stopwords.words('English')\n",
    "every_single_word = []\n",
    "for s in all_sents:\n",
    "    every_single_word = every_single_word + nltk.word_tokenize(s)\n",
    "every_single_word = [w for w in every_single_word if (w) and (w not in stopwordlist) and (w != ',')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-thousand",
   "metadata": {},
   "source": [
    "Before getting knee deep into the manipulation of the text data, let's see what the top ***n*** words are by raw count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bronze-clinic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words by frequency/count: \n",
      " [('employees', 6), ('financial', 4), ('training', 3), ('good', 3), ('shares', 2)]\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "topcommonwords = Counter(every_single_word).most_common()\n",
    "topNwords = topcommonwords[:N]\n",
    "print(f'Top {N} words by frequency/count: \\n', topNwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-ambassador",
   "metadata": {},
   "source": [
    "**Now let's construct the bag of words by tokenizing the raw sentences.** \n",
    "\n",
    "As an example, imagine my sentence is 'Obiwan is actually Luke's father. Anakin would be devastated'.\n",
    "\n",
    "From the **all_sents** that we constructed, the input list would look something *like* this:\n",
    "\n",
    "    all_sents = ['Obiwan is actually Luke's father',\n",
    "                'Anakin would be devastated']\n",
    "    \n",
    "    and the output would be in a form similar to this (a list of word sets derived from sentences):\n",
    "    wordsinabag = [ {'Obiwan', 'actually', 'Luke', 'father'},\n",
    "                   {'Anakin', 'devastated'}]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "psychological-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentence and clean it as well\n",
    "def tokenize_sentence(s):\n",
    "    from re import findall\n",
    "    pattern = r\"[a-zA-Z]+\"\n",
    "    res = re.findall(pattern, s)\n",
    "    out = [w for w in res if w]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "printable-apartment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the bag of words. Again our goal is to return a list of sets containing the unique words which we will\n",
    "# use to evaluate the occurence of that word across all sentences. \n",
    "def bag_of_words(sents):\n",
    "    from nltk.corpus import stopwords\n",
    "    stopwordlist = stopwords.words('English')\n",
    "    x = [tokenize_sentence(s) for s in sents]\n",
    "    out = []\n",
    "    for cleaned_sentence in x:\n",
    "        out.append({word for word in cleaned_sentence if word not in stopwordlist})\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "upset-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsinabag = bag_of_words(all_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "silent-disco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the first 5 elements (tokenized sentences): \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'pension'},\n",
       " {'development', 'k', 'match', 'opportunities', 'permits', 'time', 'training'},\n",
       " {'actually',\n",
       "  'bond',\n",
       "  'communities',\n",
       "  'difference',\n",
       "  'edge',\n",
       "  'green',\n",
       "  'initiatives',\n",
       "  'investing',\n",
       "  'leading',\n",
       "  'making',\n",
       "  'social'},\n",
       " {'capable', 'employees', 'especially', 'flexible', 'pandemic', 'remote'},\n",
       " {'change', 'employees', 'leadership', 'listen', 'shown', 'willingness'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Here are the first 5 elements (tokenized sentences): \")\n",
    "wordsinabag[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-monroe",
   "metadata": {},
   "source": [
    "As mentioned above, I brought over the professor's code from Practice Problem 20 that generates the set of all unique words and uses this set to create word to integer mappings. We will be using these to map words and their ids to each other momentarily. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lucky-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab all of the unique words from the bag of words constructed above \n",
    "all_words = set()\n",
    "for b in wordsinabag:\n",
    "    all_words = all_words | b\n",
    "\n",
    "# Assigning indices to words and creating a another dictionary to \n",
    "# reverse-map the words back to their indices (later use)\n",
    "word_to_id = {w: k for k, w in enumerate(all_words)}\n",
    "id_to_word = {k: w for k, w in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-paintball",
   "metadata": {},
   "source": [
    "As a reminder, in Problem 20, the idea was to build a sparse matrix such that there are $m$ words and $n$ sentences. \n",
    "With that, we would have a matrix $A$ such that $A$ is $m \\times n$. The entries of this matrix are  $a_{i,j}$ where $i$ is the ID of a word and $j$ is the ID of the sentence (which we conveniently made the index of the sentence in the bag of words list). $n_i$ is the number of sentences that has the word of interest. Eventually we'll rank the words (you can do this with the sentences too) by how many times they appear in a setnence by leveraging the power of the SVD from Linear Algebra.\n",
    "\n",
    "\n",
    "$$\n",
    "  a_{i,j} = \\left\\{ \\begin{array}{cl}\n",
    "          \\left(\\ln \\dfrac{n+1}{n_i}\\right)^{-1} & \\mbox{if word $i$ appears in sentence $j$} \\\\\n",
    "                             0                   & \\mbox{otherwise.}\n",
    "      \\end{array} \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-veteran",
   "metadata": {},
   "source": [
    "Here is my implementation of the algorithm here in the gen_coords function (like the gen_coords function from Problem 20). Admittedly, it is not the most efficient with the nested for loop that handles the main calculation of our $a_{i,j}$ value, but it gets the job done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "median-remains",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_coords(bags, word_to_id): \n",
    "    m, n = len(word_to_id), len(bags)\n",
    "    rows, cols, vals = [], [], []\n",
    "\n",
    "    from math import log\n",
    "    def get_ni(bags, word):\n",
    "        count = 0\n",
    "        for word_set in bags:\n",
    "            if word in word_set:\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "    for i, word_set in enumerate(bags):\n",
    "        for j, word in enumerate(word_set):\n",
    "            rows.append(word_to_id[word])\n",
    "            cols.append(i)\n",
    "            ni = get_ni(bags, word)\n",
    "            want = 1/log((n + 1)/ni)\n",
    "            vals.append(want)\n",
    "        \n",
    "    return rows, cols, vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-vertex",
   "metadata": {},
   "source": [
    "Using gen_coords, we can get the row, column, and data values that will be fed into scipy's csr_matrix() solver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "agreed-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c, v = gen_coords(wordsinabag, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "common-perth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "A = csr_matrix((v, (r, c)), shape=(len(word_to_id), len(wordsinabag)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-industry",
   "metadata": {},
   "source": [
    "Now its time to actually rank the words using matrix $A$'s SVD. \n",
    "Note that we have a matrix in CSR format (you'll see why in a second). \n",
    "Using np.linalg.svd() we can easily grab the left\n",
    "singular vectors U, the singular values Sigma, and the right singular vector V.                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "extraordinary-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applySVD(A):\n",
    "    \"\"\"Apply the SVD to our problem\"\"\"\n",
    "    # Convert it back to a dense matrix\n",
    "    A = A.todense()\n",
    "    u, s, v = np.linalg.svd(A)\n",
    "    return u, s, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "random-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, V = applySVD(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "flying-spice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the largest singular - vectors U\n",
    "def get_ranked_words(U):\n",
    "    \"\"\"Takes in U, a left-singular vector such that the columns of U are the eigenvectors of AAH.\n",
    "    Returns a numpy array of the top (corresponding to the largest singular value)\n",
    "    \"\"\"\n",
    "    # Recall that the columns of u are the left-singular vectors, we want the first one corresponding to the largest\n",
    "    # singular value (the first singular value in Sigma)\n",
    "    U_0 = U[:, 0]\n",
    "    \n",
    "    # Due to the way its output, we need to take U_0, currently a matrix, and pull out its values (a list of single -element)\n",
    "    # lists containing the vector elements and store them in a numpy array\n",
    "    U_0_1 = np.array(U_0.tolist())\n",
    "    \n",
    "    # Since its a list of lists rather than a list of elements, we need to pull out the first and only element\n",
    "    # insert a print statement here if you need.\n",
    "    U_0_1 = [v[0] for v in U_0_1]\n",
    "    \n",
    "    # We are going to store it in an array and use argsort to get the indices of the sorted array\n",
    "    U_0_1 = np.array(U_0_1)\n",
    "    return np.argsort(U_0_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "normal-bunny",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 words computed: ['employees', 'good', 'training', 'culture', 'bank']\n"
     ]
    }
   ],
   "source": [
    "# Now let's rank the words\n",
    "ranked_words = get_ranked_words(U)\n",
    "top_ten_words = [id_to_word[k] for k in ranked_words[:5]]\n",
    "print(f\"Top 5 words computed:\", top_ten_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-biotechnology",
   "metadata": {},
   "source": [
    "**For demonstration purposes, this is Professor Vuduc's implementation in Problem 20 to get the same results from above.** \n",
    "\n",
    "It turns out that scipy.sparse has a svds() solver that can be optioned to get the largest singular vectors -- the 'LM' argument -- and also choose how many singular values (and their corresponding singular vectors to return --the k=1 argument --). A major difference here is that, the svds() function takes in as input a sparse matrix unlike my implementation above which needed a dense matrix to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acute-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svds_largest(A):\n",
    "    from scipy.sparse.linalg import svds\n",
    "    from numpy import abs\n",
    "    u, s, v = svds(A, k=1, which='LM', return_singular_vectors=True)\n",
    "    return s, abs(u.reshape(A.shape[0])), abs(v.reshape(A.shape[1]))\n",
    "\n",
    "_, u0, _ = get_svds_largest(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "prepared-anime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words computed: ['employees', 'good', 'training', 'culture', 'bank']\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "ranked_words = np.argsort(u0)[::-1]\n",
    "top_ten_words = [id_to_word[k] for k in ranked_words[:n]]\n",
    "print(f\"Top {n} words computed:\", top_ten_words)\n",
    "a = set(top_ten_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sunset-proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words by count: ['employees', 'financial', 'training', 'good', 'shares']\n"
     ]
    }
   ],
   "source": [
    "b = [w[0] for w in topcommonwords[:n]]\n",
    "print(f\"Top {n} words by count:\", b)\n",
    "b = set(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "informative-shark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From the naked eye we can see how similar these sets are by the elements in their intersection: \n",
      " {'good', 'employees', 'training'}. <- Here we have 3 elements out of a total of 5.\n"
     ]
    }
   ],
   "source": [
    "print(f'From the naked eye we can see how similar these sets are by the \\\n",
    "elements in their intersection: \\n {a&b}. <- Here we have {len(a&b)} elements out of a total of {len(a)}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-assault",
   "metadata": {},
   "source": [
    "### What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-paintball",
   "metadata": {},
   "source": [
    "Using regex again, let's pull in job title information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "arabic-gothic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>flag</th>\n",
       "      <th>jobtitles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pension. 401k match, training and development ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Technical Writer/Editor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>especially with the pandemic, they have been v...</td>\n",
       "      <td>1</td>\n",
       "      <td>Anonymous Employee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i was able to do some financial planning for t...</td>\n",
       "      <td>0</td>\n",
       "      <td>Anonymous Employee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bank recognizes value of technology and ongoin...</td>\n",
       "      <td>1</td>\n",
       "      <td>Treasury Management Officer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my best reason for working there is having the...</td>\n",
       "      <td>0</td>\n",
       "      <td>Financial Consultant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>they pay is decent enough i guess</td>\n",
       "      <td>0</td>\n",
       "      <td>Branch Banker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>decent culture. seems to care about employees....</td>\n",
       "      <td>1</td>\n",
       "      <td>Senior Manager, Product Management</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>interacting with customers, fun co workers, go...</td>\n",
       "      <td>0</td>\n",
       "      <td>Customer Service Representative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>excellent, diverse financial institute. that l...</td>\n",
       "      <td>1</td>\n",
       "      <td>Account Executive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pnc is a well-established organization and ple...</td>\n",
       "      <td>1</td>\n",
       "      <td>Campus Recruiter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  flag  \\\n",
       "0  pension. 401k match, training and development ...     0   \n",
       "1  especially with the pandemic, they have been v...     1   \n",
       "2  i was able to do some financial planning for t...     0   \n",
       "3  bank recognizes value of technology and ongoin...     1   \n",
       "4  my best reason for working there is having the...     0   \n",
       "5                  they pay is decent enough i guess     0   \n",
       "6  decent culture. seems to care about employees....     1   \n",
       "7  interacting with customers, fun co workers, go...     0   \n",
       "8  excellent, diverse financial institute. that l...     1   \n",
       "9  pnc is a well-established organization and ple...     1   \n",
       "\n",
       "                            jobtitles  \n",
       "0             Technical Writer/Editor  \n",
       "1                  Anonymous Employee  \n",
       "2                  Anonymous Employee  \n",
       "3         Treasury Management Officer  \n",
       "4                Financial Consultant  \n",
       "5                       Branch Banker  \n",
       "6  Senior Manager, Product Management  \n",
       "7     Customer Service Representative  \n",
       "8                   Account Executive  \n",
       "9                    Campus Recruiter  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'reviews': outtext}\n",
    "emp_reviews = pd.DataFrame(d)\n",
    "\n",
    "\n",
    "def f(row):\n",
    "    if 'employees' in row['reviews']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "emp_reviews['flag'] = emp_reviews.apply(f, axis=1)\n",
    "\n",
    "# Regex for pulling out the job titles\n",
    "jobpattern = re.compile(r\">\\w\\w\\w \\d\\d\\, \\d\\d\\d\\d - ([\\w\\d\\s\\/,]+)<\")\n",
    "jpres = jobpattern.findall(as_string_doc)\n",
    "\n",
    "emp_reviews['jobtitles'] = pd.Series(jpres)\n",
    "emp_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-kentucky",
   "metadata": {},
   "source": [
    "Here, I also added a 'flags' column to the dataframe of reviews if a particular word had the consensus top word 'employee' (by rank as well as frequency). What can be done next is we could pull the dates of the reviews from the \n",
    "input html file. This way I could see how many times our 'top word' appeared through a specific time frame like 2020 - 2021.\n",
    "\n",
    "Could it be the case that the top concern for employees have changed from say, benefits to a better work life balance since the start\n",
    "of the pandemic? Or is there a new theme that is yet to be uncovered?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-longitude",
   "metadata": {},
   "source": [
    "Or maybe further analysis could be more targeted to verifying if whether or not a theme does indeed exist around say -pay and compensation. Is that consistently appearing in the top 'x' number of comments?\n",
    "Are key words like 'balance' or 'work-life' or 'flexibility' showing up in the rankings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-hostel",
   "metadata": {},
   "source": [
    "If there are indicators like 'work-life' and 'flexibility' could we use a flag to see if the distribution of reviews with that flag\n",
    "is proportional across job families or job titles? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
